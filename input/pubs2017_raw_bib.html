<h1>pubs2017.bib</h1><a name="morfi2017deductiverecordings"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#morfi2017deductiverecordings">morfi2017deductiverecordings</a>,
  title = {Deductive Refinement of Species Labelling in Weakly Labelled Birdsong Recordings},
  author = {Morfi, V and STOWELL, DF},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP2017 )},
  year = {2017},
  organization = {New Orleans, USA},
  abstract = {Many approaches have been used in bird species classification from their sound in order to provide labels for the whole of a recording. However, a more precise classification of each bird vocalization would be of great importance to the use and management of sound archives and bird monitoring. In this work, we introduce a technique that using a two step process can first automatically detect all bird vocalizations and then, with the use of âweaklyâ labelled recordings, classify them. Evaluations of our proposed method show that it achieves a correct classification of 75.4\% when used in a synthetic dataset.},
  finishday = {9},
  finishmonth = {Mar},
  finishyear = {2017},
  publicationstatus = {published},
  startday = {5},
  startmonth = {Mar},
  startyear = {2017}
}
</pre>

<a name="zacharakis2017rearrangementcorrelates"></a><pre>
@article{<a href="pubs2017_raw.html#zacharakis2017rearrangementcorrelates">zacharakis2017rearrangementcorrelates</a>,
  title = {Rearrangement of Timbre Space Due To Background Noise: Behavioural Evidence and Acoustic Correlates},
  author = {Zacharakis, A and Terrell, M and Simpson, AR and Pastiadis, K and Reiss, J},
  journal = {Acta Acustica united with Acustica},
  year = {2017},
  month = {Mar},
  pages = {288--298},
  volume = {103},
  day = {1},
  doi = {10.3813/AAA.919057},
  issn = {1610-1928},
  issue = {2},
  publicationstatus = {published}
}
</pre>

<a name="reiss2017userparameters"></a><pre>
@article{<a href="pubs2017_raw.html#reiss2017userparameters">reiss2017userparameters</a>,
  title = {User Preference on Artificial Reverberation and Delay Time Parameters},
  author = {REISS, J},
  journal = {Journal of the Audio Engineering Society},
  year = {2017},
  month = {Feb},
  day = {16},
  issn = {1549-4950},
  publicationstatus = {published},
  publisher = {Audio Engineering Society}
}
</pre>

<a name="reiss2017perceptualproduction"></a><pre>
@article{<a href="pubs2017_raw.html#reiss2017perceptualproduction">reiss2017perceptualproduction</a>,
  title = {Perceptual evaluation and analysis of reverberation in multitrack music production},
  author = {REISS, J and De Man, B},
  journal = {Journal of the Audio Engineering Society},
  year = {2017},
  month = {Feb},
  day = {16},
  doi = {10.17743/jaes.2016.0062},
  issn = {1549-4950},
  publicationstatus = {published},
  publisher = {Audio Engineering Society}
}
</pre>

<a name="halpern2017thatexpectation."></a><pre>
@article{<a href="pubs2017_raw.html#halpern2017thatexpectation.">halpern2017thatexpectation.</a>,
  title = {That note sounds wrong! Age-related effects in processing of musical expectation.},
  author = {Halpern, AR and Zioga, I and Shankleman, M and Lindsen, J and Pearce, MT and Bhattacharya, J},
  journal = {Brain Cogn},
  year = {2017},
  month = {Apr},
  pages = {1--9},
  volume = {113},
  abstract = {Part of musical understanding and enjoyment stems from the ability to accurately predict what note (or one of a small set of notes) is likely to follow after hearing the first part of a melody. Selective violation of expectations can add to aesthetic response but radical or frequent violations are likely to be disliked or not comprehended. In this study we investigated whether a lifetime of exposure to music among untrained older adults would enhance their reaction to unexpected endings of unfamiliar melodies. Older and younger adults listened to melodies that had expected or unexpected ending notes, according to Western music theory. Ratings of goodness-of-fit were similar in the groups, as was ERP response to the note onset (N1). However, in later time windows (P200 and Late Positive Component), the amplitude of a response to unexpected and expected endings was both larger in older adults, corresponding to greater sensitivity, and more widespread in locus, consistent with a dedifferentiation pattern. Lateralization patterns also differed. We conclude that older adults refine their understanding of this important aspect of music throughout life, with the ability supported by changing patterns of neural activity.},
  doi = {10.1016/j.bandc.2016.12.006},
  eissn = {1090-2147},
  keyword = {Aging},
  language = {eng},
  organization = {United States},
  pii = {S0278-2626(16)30043-4},
  publicationstatus = {published},
  url = {https://www.ncbi.nlm.nih.gov/pubmed/28064077}
}
</pre>

<a name="agres2017information-theoreticmemory"></a><pre>
@article{<a href="pubs2017_raw.html#agres2017information-theoreticmemory">agres2017information-theoreticmemory</a>,
  title = {Information-Theoretic Properties of Auditory Sequences Dynamically Influence Expectation and Memory},
  author = {Agres, K and Abdallah, S and Pearce, M},
  journal = {Cognitive Science},
  year = {2017},
  month = {Jan},
  day = {25},
  doi = {10.1111/cogs.12477},
  issn = {0364-0213},
  publicationstatus = {published}
}
</pre>

<a name="morreale2017buildingplatform"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#morreale2017buildingplatform">morreale2017buildingplatform</a>,
  title = {Building a Maker Community Around an Open Hardware Platform},
  author = {Morreale, F and Moro, G and Chamberlain, A and Benford, S and MCPHERSON, A},
  booktitle = {ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)},
  year = {2017},
  month = {May},
  organization = {Denver, CO, USA},
  abstract = {This paper reflects on the dynamics and practices of building a maker community around a new hardware platform. We examine the factors promoting the successful uptake of a maker platform from two perspectives: first, we investigate the technical and user experience considerations that users identify as the most important. Second, we explore the specific activities that help attract a community and encourage sustained participation. We present an inductive approach based on the case study of Bela, an embedded platform for creating interactive audio systems. The technical design and community building processes are detailed, culminating in a successful crowdfunding campaign. To further understand the community dynamics, the paper also presents an intensive three-day workshop with eight digital musical instrument designers. From observations and interviews, we reflect on the relationship between the platform and the community and offer suggestions for HCI researchers and practitioners interested in establishing their own maker communities.},
  day = {6},
  finishday = {11},
  finishmonth = {May},
  finishyear = {2017},
  publicationstatus = {accepted},
  startday = {6},
  startmonth = {May},
  startyear = {2017}
}
</pre>

<a name="herremans2017atension"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#herremans2017atension">herremans2017atension</a>,
  title = {A multi-modal platform for semantic music analysis: visualizing audio- and score-based tension},
  author = {HERREMANS, D and Chuan, CH},
  booktitle = {IEEEInternational Conference on Semantic Computing},
  year = {2017},
  month = {Jan},
  organization = {San Diego},
  day = {29},
  finishday = {1},
  finishmonth = {Feb},
  finishyear = {2017},
  publicationstatus = {published},
  startday = {30},
  startmonth = {Jan},
  startyear = {2017}
}
</pre>

<a name="agres2017harmonicmusic"></a><pre>
@article{<a href="pubs2017_raw.html#agres2017harmonicmusic">agres2017harmonicmusic</a>,
  title = {Harmonic Structure Predicts the Enjoyment of Uplifting Trance Music},
  author = {Agres, K and Herremans, D and Bigo, L and Conklin, D},
  journal = {Frontiers in Psychology},
  year = {2017},
  month = {Jan},
  volume = {7},
  abstract = {Â© 2017 Agres, Herremans, Bigo and Conklin.An empirical investigation of how local harmonic structures (e.g., chord progressions) contribute to the experience and enjoyment of uplifting trance (UT) music is presented. The connection between rhythmic and percussive elements and resulting trance-like states has been highlighted by musicologists, but no research, to our knowledge, has explored whether repeated harmonic elements influence affective responses in listeners of trance music. Two alternative hypotheses are discussed, the first highlighting the direct relationship between repetition/complexity and enjoyment, and the second based on the theoretical inverted-U relationship described by the Wundt curve. We investigate the connection between harmonic structure and subjective enjoyment through interdisciplinary behavioral and computational methods: First we discuss an experiment in which listeners provided enjoyment ratings for computer-generated UT anthems with varying levels of harmonic repetition and complexity. The anthems were generated using a statistical model trained on a corpus of 100 uplifting trance anthems created for this purpose, and harmonic structure was constrained by imposing particular repetition structures (semiotic patterns defining the order of chords in the sequence) on a professional UT music production template. Second, the relationship between harmonic structure and enjoyment is further explored using two computational approaches, one based on average Information Content, and another that measures average tonal tension between chords. The results of the listening experiment indicate that harmonic repetition does in fact contribute to the enjoyment of uplifting trance music. More compelling evidence was found for the second hypothesis discussed above, however some maximally repetitive structures were also preferred. Both computational models provide evidence for a Wundt-type relationship between complexity and enjoyment. By systematically manipulating the structure of chord progressions, we have discovered specific harmonic contexts in which repetitive or complex structure contribute to the enjoyment of uplifting trance music.},
  day = {10},
  doi = {10.3389/fpsyg.2016.01999},
  eissn = {1664-1078},
  issue = {JAN},
  publicationstatus = {published}
}
</pre>

<a name="benetos2017polyphonicsystems"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#benetos2017polyphonicsystems">benetos2017polyphonicsystems</a>,
  title = {Polyphonic note and instrument tracking using linear dynamical systems},
  author = {Benetos, E},
  booktitle = {2017 AES International Conference on Semantic Audio},
  year = {2017},
  month = {Jun},
  organization = {Erlangen, Germany},
  publisher = {Audio Engineering Society},
  abstract = {In this paper, a system for automatic transcription of multiple-instrument polyphonic music is proposed, which supports tracking multiple concurrent notes using linear dynamical systems (LDS). The system is based on a spectrogram factorisation model which extends probabilistic latent component analysis (PLCA), and supports the detection of multiple pitches, instrument contributions, and pitch deviations. In order to jointly track multiple concurrent pitches, the use of LDS as prior to the PLCA model is proposed. LDS parameters are learned in a training stage using score-informed transcriptions; for LDS inference, online and offline variants are evaluated. The MAPS piano music dataset and the Bach10 multi-instrument dataset are used for note tracking experiments, with the latter dataset also being evaluated with respect to instrument assignment performance. Results show that the proposed LDS-based method can successfully track multiple concurrent notes, leading to an improvement of over 3\% in terms of note-based F-measure for both datasets over benchmark note tracking approaches.},
  day = {22},
  finishday = {24},
  finishmonth = {Jun},
  finishyear = {2017},
  publicationstatus = {accepted},
  startday = {22},
  startmonth = {Jun},
  startyear = {2017},
  url = {<a href="http://www.aes.org/conferences/2017/semantic/">http://www.aes.org/conferences/2017/semantic/</a>}
}
</pre>

<a name="valero-mas2017assessingtranscription"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#valero-mas2017assessingtranscription">valero-mas2017assessingtranscription</a>,
  title = {Assessing the Relevance of Onset Information for Note Tracking in Piano Music Transcription},
  author = {Valero-Mas, JJ and Benetos, E and IÃ±esta, JM},
  booktitle = {2017 AES International Conference on Semantic Audio},
  year = {2017},
  month = {Jun},
  organization = {Erlangen, Germany},
  publisher = {Audio Engineering Society},
  day = {22},
  finishday = {24},
  finishmonth = {Jun},
  finishyear = {2017},
  publicationstatus = {accepted},
  startday = {22},
  startmonth = {Jun},
  startyear = {2017},
  url = {<a href="http://www.aes.org/conferences/2017/semantic/">http://www.aes.org/conferences/2017/semantic/</a>}
}
</pre>

<a name="russell2017onmodels"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#russell2017onmodels">russell2017onmodels</a>,
  title = {On the Memory Properties of Recurrent Neural Models},
  author = {Russell, AJ and Benetos, E and d'Avila Garcez, AS},
  booktitle = {International Joint Conference on Neural Networks (IJCNN 2017)},
  year = {2017},
  month = {May},
  organization = {Anchorage, Alaska, USA},
  abstract = {In this paper, we investigate the memory properties of two popular gated units: long short term memory (LSTM) and gated recurrent units (GRU), which have been used in recurrent neural networks (RNN) to achieve state-of-the-art performance on several machine learning tasks. We propose five basic tasks for isolating and examining specific capabilities relating to the implementation of memory. Results show that (i) both types of gated unit perform less reliably than standard RNN units on tasks testing fixed delay recall, (ii) the reliability of stochastic gradient descent decreases as network complexity increases, and (iii) gated units are found to perform better than standard RNNs on tasks that require values to be stored in memory and updated conditionally upon input to the network. Task performance is found to be surprisingly independent of network depth (number of layers) and connection architecture. Finally, visualisations of the solutions found by these networks are presented and explored, exposing for the first time how logic operations are implemented by individual gated cells and small groups of these cells.},
  day = {14},
  finishday = {14},
  finishmonth = {May},
  finishyear = {2017},
  publicationstatus = {accepted},
  startday = {19},
  startmonth = {May},
  startyear = {2017}
}
</pre>

<a name="abdallah2017themusicology"></a><pre>
@article{<a href="pubs2017_raw.html#abdallah2017themusicology">abdallah2017themusicology</a>,
  title = {The Digital Music Lab: A Big Data Infrastructure for Digital Musicology},
  author = {Abdallah, S and Benetos, E and Gold, N and Hargreaves, S and Weyde, T and Wolff, D},
  journal = {ACM Journal on Computing and Cultural Heritage},
  year = {2017},
  month = {Jan},
  volume = {10},
  day = {1},
  doi = {10.1145/2983918},
  issn = {1556-4673},
  issue = {1},
  publicationstatus = {published},
  publisher = {ACM},
  url = {<a href="http://jocch.acm.org/">http://jocch.acm.org/</a>}
}
</pre>

<a name="goddard2017designingsystems"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#goddard2017designingsystems">goddard2017designingsystems</a>,
  title = {Designing Computationally Creative Musical Performance Systems},
  author = {Goddard, C and BARTHET, M and Wiggins, G},
  booktitle = {dl.acm.org},
  year = {2017},
  month = {Aug},
  organization = {Queen Mary University of London, London},
  publisher = {ACM},
  volume = {Proceedings of AM ’17},
  abstract = {This is work in progress where we outline a design process
for a computationally creative musical performance system
using the Creative Systems Framework (CSF). The proposed
system is intended to produce virtuosic interpretations, and
subsequent synthesized renderings of these interpretations
with a physical model of a bass guitar, using case-based
reasoning and reflection. We introduce our interpretations
of virtuosity and musical performance, outline the suitability
of case-based reasoning in computationally creative systems
and introduce notions of computational creativity and the
CSF. We design our system by formalising the components
of the CSF and briefly outline a potential implementation.
In doing so, we demonstrate how the CSF can be used as
a tool to aid in designing computationally creative musical
performance systems.},
  conference = {Audio Mostly 2017},
  day = {23},
  doi = {10.1145/3123514.3123541},
  finishday = {26},
  finishmonth = {Aug},
  finishyear = {2017},
  isbn = {978-1-4503-5373-1},
  keyword = {computational creativity},
  publicationstatus = {published},
  startday = {23},
  startmonth = {Aug},
  startyear = {2017},
  timestamp = {2018.02.05},
  url = {<a href="http://callumgoddard.com/">http://callumgoddard.com/</a>}
}
</pre>

<a name="olowe2017featuruxavaudio"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#olowe2017featuruxavaudio">olowe2017featuruxavaudio</a>,
  title = {FEATUR.UX.AV: A live sound visualization system using multitrack audio},
  author = {Olowe, I and Grierson, M and Barthet, M},
  booktitle = {ACM International Conference Proceeding Series},
  year = {2017},
  month = {Aug},
  volume = {Part F131930},
  abstract = {© 2017 Copyright held by the owner/author(s). In this paper, we describe the conceptual design and technical implementation of an audiovisual system whereby multitrack audio is used to generate visualizations in real time. We discuss our motivation within the context of audiovisual practice and present the outcomes of studies conducted to outline design requirements. We then describe the audio and visual components of our multitrack visualization model, and specific parts of the graphical user interface (GUI) which focus on mapping as the primary mechanism to facilitate live multitrack audiovisual performance.},
  day = {23},
  doi = {10.1145/3123514.3123561},
  isbn = {9781450353731},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="olowe2017useraudio"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#olowe2017useraudio">olowe2017useraudio</a>,
  title = {User requirements for live sound visualization system using multitrack audio},
  author = {Olowe, I and Grierson, M and Barthet, M},
  booktitle = {ACM International Conference Proceeding Series},
  year = {2017},
  month = {Aug},
  volume = {Part F131930},
  abstract = {© 2017 Copyright held by the owner/author(s). In this paper, we identify design requirements for a screen-based system that enables live sound visualization using multitrack audio. Our mixed methodology is grounded in user-centered design and involved a review of the literature to assess the state-of-the-art of Video Jockeying (VJing), and two online surveys to canvas practices within the audiovisual community and gain practical and aspirational awareness on the subject. We review ten studies about VJ practice and culture and human computer interaction topics within live performance. Results from the first survey, completed by 22 participants, were analysed to identify general practices, mapping preferences, and impressions about multitrack audio and audio-content feature extraction. A second complementary survey was designed to probe about specific implications of performing with a system that facilitates live visual performance using multitrack audio. Analyses from 29 participants' self-reports highlight that the creation of audiovisual content is a multivariate and subjective process and help define where multitrack audio, audio-content extraction, and live mapping could fit within. We analyze the findings and discuss how they can inform a design for our system.},
  day = {23},
  doi = {10.1145/3123514.3123527},
  isbn = {9781450353731},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="schramm2017automaticsingers"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#schramm2017automaticsingers">schramm2017automaticsingers</a>,
  title = {Automatic Transcription of a Cappella Recordings from Multiple Singers},
  author = {Schramm, R and Benetos, E},
  year = {2017},
  month = {Jun},
  organization = {Erlangen, Germany},
  publisher = {Audio Engineering Society},
  abstract = {This work presents a spectrogram factorisation method applied to automatic music transcription of a cappella performances with multiple singers. A variable-Q transform representation of the audio spectrogram is factorised with the help of a 6-dimensional sparse dictionary which contains spectral templates of vowel vocalizations. A post-processing step is proposed to remove false positive pitch detections through a binary classifier, where overtone-based features are used as input into this step. Preliminary experiments have shown promising multi-pitch detection results when applied to audio recordings of Bach Chorales and Barbershop music. Comparisons made with alternative methods have shown that our approach increases the number of true positive pitch detections while the post-processing step keeps the number of false positives lower than those measured in comparative approaches.},
  conference = {2017 AES International Conference on Semantic Audio},
  day = {22},
  doi = {10.17743/aesconf.2017.978-1-942220-15-2},
  finishday = {24},
  finishmonth = {Jun},
  finishyear = {2017},
  publicationstatus = {published},
  startday = {22},
  startmonth = {Jun},
  startyear = {2017},
  timestamp = {2018.02.05},
  url = {<a href="http://professor.ufrgs.br/rschramm">http://professor.ufrgs.br/rschramm</a>}
}
</pre>

<a name="lafay2017soundresults"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#lafay2017soundresults">lafay2017soundresults</a>,
  title = {Sound Event Detection in Synthetic Audio: Analysis of the DCASE 2016 Task Results},
  author = {Lafay, G and Benetos, E and Lagrange, M},
  booktitle = {<a href="http://www.waspaa.com/">http://www.waspaa.com/</a>},
  year = {2017},
  month = {Oct},
  organization = {New Paltz, NY, USA},
  pages = {11--15},
  publisher = {IEEE},
  abstract = {As part of the 2016 public evaluation challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2016), the second task focused on evaluating sound event detection systems using synthetic mixtures of office sounds. This task, which follows the 'Event Detection - Office Synthetic' task of DCASE 2013, studies the behaviour of tested algorithms when facing controlled levels of audio complexity with respect to background noise and polyphony/density, with the added benefit of a very accurate ground truth. This paper presents the task formulation, evaluation metrics, submitted systems, and provides a statistical analysis of the results achieved, with respect to various aspects of the evaluation dataset.},
  conference = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA 2017)},
  day = {15},
  doi = {10.1109/WASPAA.2017.8169985},
  finishday = {15},
  finishmonth = {Oct},
  finishyear = {2017},
  publicationstatus = {published},
  startday = {18},
  startmonth = {Oct},
  startyear = {2017},
  timestamp = {2018.02.05},
  url = {<a href="http://www.waspaa.com/">http://www.waspaa.com/</a>}
}
</pre>

<a name="ycart2017amodelling"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#ycart2017amodelling">ycart2017amodelling</a>,
  title = {A study on LSTM networks for polyphonic music sequence modelling},
  author = {Ycart, A and Benetos, E},
  year = {2017},
  month = {Oct},
  organization = {Suzhou, China},
  pages = {421--427},
  publisher = {ISMIR},
  abstract = {Neural networks, and especially long short-term memory networks (LSTM), have become increasingly popular for sequence modelling, be it in text, speech, or music. In this paper, we investigate the predictive power of simple LSTM networks for polyphonic MIDI sequences, using an empirical approach. Such systems can then be used as a music language model which, combined with an acoustic model, can improve automatic music transcription (AMT) performance. As a first step, we experiment with synthetic MIDI data, and we compare the results obtained in various settings, throughout the training process. In particular, we compare the use of a fixed sample rate against a musically-relevant sample rate. We test this system both on synthetic and real MIDI data. Results are compared in terms of note prediction accuracy. We show that the higher the sample rate is, the better the prediction is, because self transitions are more frequent. We suggest that for AMT, a musically-relevant sample rate is crucial in order to model note transitions, beyond a simple smoothing effect.},
  conference = {18th International Society for Music Information Retrieval Conference (ISMIR 2017)},
  day = {23},
  finishday = {27},
  finishmonth = {Oct},
  finishyear = {2017},
  publicationstatus = {accepted},
  startday = {23},
  startmonth = {Oct},
  startyear = {2017},
  timestamp = {2018.02.05},
  url = {<a href="http://www.eecs.qmul.ac.uk/~ay304/">http://www.eecs.qmul.ac.uk/~ay304/</a>}
}
</pre>

<a name="schramm2017multipitchsingers"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#schramm2017multipitchsingers">schramm2017multipitchsingers</a>,
  title = {Multi-pitch detection and voice assignment for a cappella recordings of multiple singers},
  author = {Schramm, R and McLeod, A and Steedman, M and Benetos, E},
  year = {2017},
  month = {Oct},
  organization = {Suzhou, China},
  pages = {552--559},
  publisher = {ISMIR},
  abstract = {This paper presents a multi-pitch detection and voice assignment method applied to audio recordings containing a cappella performances with multiple singers. A novel approach combining an acoustic model for multi-pitch detection and a music language model for voice separation and assignment is proposed. The acoustic model is a spectrogram factorization process based on Probabilistic Latent Component Analysis (PLCA), driven by a 6-dimensional dictionary with pre-learned spectral templates. The voice separation component is based on hidden Markov models that use musicological assumptions. By integrating the models, the system can detect multiple concurrent pitches in vocal music and assign each detected pitch to a specific voice corresponding to a voice type such as soprano, alto, tenor or bass (SATB). This work focuses on four-part compositions, and evaluations on recordings of Bach Chorales and Barbershop quartets show that our integrated approach achieves an F-measure of over 70\% for frame-based multi-pitch detection and over 45\% for four-voice assignment.},
  conference = {18th International Society for Music Information Retrieval Conference (ISMIR 2017)},
  day = {23},
  finishday = {27},
  finishmonth = {Oct},
  finishyear = {2017},
  publicationstatus = {accepted},
  startday = {23},
  startmonth = {Oct},
  startyear = {2017},
  timestamp = {2018.02.05},
  url = {https://ismir2017.smcnus.org/}
}
</pre>

<a name="stowell2017onbirdcontexts"></a><pre>
@article{<a href="pubs2017_raw.html#stowell2017onbirdcontexts">stowell2017onbirdcontexts</a>,
  title = {On-bird Sound Recordings: Automatic Acoustic Recognition of Activities and Contexts},
  author = {Stowell, D and Benetos, E and Gill, LF},
  journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing},
  year = {2017},
  month = {May},
  pages = {1193--1206},
  volume = {25},
  abstract = {We introduce a novel approach to studying animal behaviour and the context in which it occurs, through the use of microphone backpacks carried on the backs of individual free-flying birds. These sensors are increasingly used by animal behaviour researchers to study individual vocalisations of freely behaving animals, even in the field. However such devices may record more than an animals vocal behaviour, and have the potential to be used for investigating specific activities (movement) and context (background) within which vocalisations occur. To facilitate this approach, we investigate the automatic annotation of such recordings through two different sound scene analysis paradigms: a scene-classification method using feature learning, and an event-detection method using probabilistic latent component analysis (PLCA). We analyse recordings made with Eurasian jackdaws (Corvus monedula) in both captive and field settings. Results are comparable with the state of the art in sound scene analysis; we find that the current recognition quality level enables scalable automatic annotation of audio logger data, given partial annotation, but also find that individual differences between animals and/or their backpacks limit the generalisation from one individual to another. we consider the interrelation of 'scenes' and 'events' in this particular task, and issues of temporal resolution.},
  day = {23},
  doi = {10.1109/TASLP.2017.2690565},
  issn = {2329-9290},
  issue = {6},
  keyword = {cs.SD},
  publicationstatus = {published},
  publisher = {IEEE},
  timestamp = {2018.02.05},
  url = {<a href="http://arxiv.org/abs/1612.05489v1">http://arxiv.org/abs/1612.05489v1</a>}
}
</pre>

<a name="benetos2017polyphonicsystemsj"></a><pre>
@article{<a href="pubs2017_raw.html#benetos2017polyphonicsystemsj">benetos2017polyphonicsystemsj</a>,
  title = {Polyphonic Sound Event Tracking using Linear Dynamical Systems},
  author = {Benetos, E and Lafay, G and Lagrange, M and Plumbley, MD},
  journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing},
  year = {2017},
  month = {May},
  pages = {1266--1277},
  volume = {25},
  abstract = {In this paper, a system for polyphonic sound event detection and tracking is proposed, based on spectrogram factorisation techniques and state space models. The system extends probabilistic latent component analysis (PLCA) and is modelled around a 4-dimensional spectral template dictionary of frequency, sound event class, exemplar index, and sound state. In order to jointly track multiple overlapping sound events over time, the integration of linear dynamical systems (LDS) within the PLCA inference is proposed. The system assumes that the PLCA sound event activation is the (noisy) observation in an LDS, with the latent states corresponding to the true event activations. LDS training is achieved using fully observed data, making use of ground truth-informed event activations produced by the PLCA-based model. Several LDS variants are evaluated, using polyphonic datasets of office sounds generated from an acoustic scene simulator, as well as real and synthesized monophonic datasets for comparative purposes. Results show that the integration of LDS tracking within PLCA leads to an improvement of +8.5-10.5\% in terms of frame-based F-measure as compared to the use of the PLCA model alone. In addition, the proposed system outperforms several state-of-the-art methods for the task of polyphonic sound event detection.},
  day = {23},
  doi = {10.1109/TASLP.2017.2690576},
  issn = {2329-9304},
  issue = {6},
  publicationstatus = {published},
  publisher = {IEEE},
  timestamp = {2018.02.05},
  url = {<a href="http://ieeexplore.ieee.org/document/7933041/">http://ieeexplore.ieee.org/document/7933041/</a>}
}
</pre>

<a name="bin2017inthemomentperformance"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#bin2017inthemomentperformance">bin2017inthemomentperformance</a>,
  title = {In-the-moment and beyond: Combining post-hoc and real time data for the study of audience perception of electronic music performance},
  author = {BIN, SMA and MORREALE, F and BRYAN-KINNS, N and MCPHERSON, A},
  year = {2017},
  month = {Sep},
  organization = {Mumbai, India},
  abstract = {This paper presents a methodology for the study of audience perception of live performances, using a combined approach of post-hoc and real-time data. We conducted a study that queried audience enjoyment and their perception of error in digital musical instrument (DMI) performance. We collected quantitative and qualitative data from the participants (N=64) via paper survey after each performance and at the end of the concert, and during the performances spectators were invited to indicate moments of enjoyment and incidences of error using a two-button mobile app interface. We demonstrate that real-time indication of error does not translate to reported non-enjoyment and post-hoc and real-time data sets are not necessarily consistent for each participant. In conclusion we make the case for a combined approach to audience studies in live performance contexts.},
  conference = {ACM Interact 2017},
  day = {25},
  finishday = {29},
  finishmonth = {Sep},
  finishyear = {2017},
  publicationstatus = {accepted},
  startday = {25},
  startmonth = {Sep},
  startyear = {2017},
  timestamp = {2018.02.05}
}
</pre>

<a name="bin2017handsperception"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#bin2017handsperception">bin2017handsperception</a>,
  title = {Hands where we can see them! Investigating the impact of gesture size on audience perception},
  author = {Bin, SMA and Bryan-Kinns, N and McPherson, A},
  year = {2017},
  month = {Oct},
  organization = {Shanghai, China},
  abstract = {This paper explores the relative effect of gesture size on audience perception of digital musical instrument (DMI) performance. In a study involving a total audience of 28 people (split into 2 groups of 13 and 15), we used a small and large version of a DMI to examine how the size of performers' gestures might differ, and how this affects post-hoc audience ratings of enjoyment, interest and understanding, as well as their indications of `enjoyment' and `error' in real time. For each audience we held two 5-minute performances, the first on a custom-designed percussion DMI, and the second on a laptop. The DMI used in each performance was made up of three elements identical in shape, materiality, interaction and sound, but the physical size was different: For one each element was approx 12x10x5cm, and the other was about 3.5 times bigger (approx. 40x30x20cm). Data was collected both during and after the performance via post-hoc and real-time methods. We found that beyond a performance simply involving physical gesture, the size of gesture has an impact on audience ratings. In this paper we detail this study and its results, and present the implications that this finding has for DMI design.},
  conference = {International Computer Music Conference},
  day = {16},
  finishday = {20},
  finishmonth = {Oct},
  finishyear = {2017},
  publicationstatus = {accepted},
  startday = {16},
  startmonth = {Oct},
  startyear = {2017},
  timestamp = {2018.02.05}
}
</pre>

<a name="wu2017openperformances"></a><pre>
@article{<a href="pubs2017_raw.html#wu2017openperformances">wu2017openperformances</a>,
  title = {Open Symphony: Creative Participation for Audiences of Live Music Performances},
  author = {Wu, Y and Zhang, L and Bryan-Kinns, N and Barthet, M},
  journal = {IEEE Multimedia},
  year = {2017},
  month = {Feb},
  pages = {48--62},
  volume = {24},
  abstract = {© 1994-2012 IEEE. Most contemporary Western performing arts practices restrict creative interactions from audiences. Open Symphony is designed to explore audience-performer interaction in live music performances, assisted by digital technology. Audiences can conduct improvising performers by voting for various musical 'modes.' Technological components include a web-based mobile application, a visual client displaying generated symbolic scores, and a server service for the exchange of creative data. The interaction model, app, and visualization were designed through an iterative participatory design process. The system was experienced by about 120 audience and performer participants (35 completed surveys) in controlled (lab) and real-world settings. Feedback on usability and user experience was overall positive, and live interactions demonstrate significant levels of audience creative engagement. The authors identified further design challenges around audience sense of control, learnability, and compositional structure. This article is part of a special issue on multimedia for enriched music.},
  day = {9},
  doi = {10.1109/MMUL.2017.19},
  issn = {1070-986X},
  issue = {1},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="men2017thereality"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#men2017thereality">men2017thereality</a>,
  title = {The impact of transitions on user experience in virtual reality},
  author = {Men, L and Bryan-Kinns, N and Hassard, AS and Ma, Z},
  booktitle = {Proceedings - IEEE Virtual Reality},
  year = {2017},
  month = {Apr},
  pages = {285--286},
  abstract = {© 2017 IEEE. In recent years, Virtual Reality (VR) applications have become widely available. An increase in popular interest raises questions about the use of the new medium for communication. While there is a wide variety of literature regarding scene transitions in films, novels and computer games, transitions in VR are not yet widely understood. As a medium that requires a high level of immersion [2], transitions are a desirable tool. This poster delineates an experiment studying the impact of transitions on user experience of presence in VR.},
  day = {4},
  doi = {10.1109/VR.2017.7892288},
  isbn = {9781509066476},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="wu2017supportinginterfaces"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#wu2017supportinginterfaces">wu2017supportinginterfaces</a>,
  title = {Supporting non-musicians' creative engagement with musical interfaces},
  author = {Wu, Y and Bryan-Kinns, N},
  booktitle = {C and C 2017 - Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition},
  year = {2017},
  month = {Jun},
  pages = {275--286},
  abstract = {© 2017 ACM. This paper reports on a study which sets out to examine the process of creative engagement of individual non-musicians when interacting with interactive musical systems (IMSs), and to identify features of IMSs that may support non-musician's creative engagement in music making. In order to creatively engage novices, we aimed to design IMSs which would combine the intuitive interaction of 'sound toys' with the rich and complex music possibilities offered by IMSs to support individual learning and creative processes. Two IMSs were designed and built and an empirical user study was conducted of them. The study used a multi-layered methodology, which combined interviews and recordings of user interactions. Analysis of participants' behaviour with the IMSs led to the identification of interaction patterns, which were used to identify a threestep framework ('learn', 'exploration','creation') of creative engagement with the musical interfaces, and to inform the development of implications for design for supporting creative engagement with IMSs. Key implications for design identified in this study are to support novices to: learn the sound; play live; catalyze insights; and scaffold composition. The basic study methodology of this paper also offers a contribution to methods for evaluating designs by combining recording user interactions with interviews.},
  day = {22},
  doi = {10.1145/3059454.3059457},
  isbn = {9781450344036},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="yang2017filterestimation"></a><pre>
@article{<a href="pubs2017_raw.html#yang2017filterestimation">yang2017filterestimation</a>,
  title = {Filter Diagonalisation Method for Music Signal Analysis: Frame-wise Vibrato Detection and Estimation},
  author = {YANG, L and Rajab, SAYID-KHALID and Chew, E},
  journal = {Journal of Mathematics and Music},
  year = {2017},
  month = {Mar},
  day = {14},
  issn = {1745-9745},
  publicationstatus = {published},
  publisher = {Taylor \&amp; Francis: STM, Behavioural Science and Public Health Titles},
  timestamp = {2018.02.05}
}
</pre>

<a name="yang2017probabilisticmodel"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#yang2017probabilisticmodel">yang2017probabilisticmodel</a>,
  title = {Probabilistic transcription of sung melody using a pitch dynamic model},
  author = {Yang, L and Maezawa, A and Smith, JBL and Chew, E},
  booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  year = {2017},
  month = {Jun},
  pages = {301--305},
  abstract = {© 2017 IEEE. Transcribing the singing voice into music notes is challenging due to pitch fluctuations such as portamenti and vibratos. This paper presents a probabilistic transcription method for monophonic sung melodies that explicitly accounts for these local pitch fluctuations. In the hierarchical Hidden Markov Model (HMM), an upper-level ergodic HMM handles the transitions between notes, and a lower-level left-to-right HMM handles the intra- and inter-note pitch fluctuations. The lower-level HMM employs the pitch dynamic model, which explicitly expresses the pitch curve characteristics as the observation likelihood over ∞ 0 and Δ∞ 0 using a compact parametric distribution. A histogram-based tuning frequency estimation method, and some post-processing heuristics to separate merged notes and to allocate spuriously detected short notes, improve the note recognition performance. With model parameters that support intuitions about singing behavior, the proposed method obtained encouraging results when evaluated on a published monophonic sung melody dataset, and compared with state-of-the-art methods.},
  day = {16},
  doi = {10.1109/ICASSP.2017.7952166},
  isbn = {9781509041176},
  issn = {1520-6149},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="herremans2017immaemoannotations"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#herremans2017immaemoannotations">herremans2017immaemoannotations</a>,
  title = {IMMA-Emo: A multimodal interface for visualising score- and audio-synchronised emotion annotations},
  author = {Herremans, D and Yang, S and Chuan, CH and Barthet, M and Chew, E},
  booktitle = {ACM International Conference Proceeding Series},
  year = {2017},
  month = {Aug},
  volume = {Part F131930},
  abstract = {© 2017 Copyright held by the owner/author(s). Emotional response to music is often represented on a two-dimensional arousal-valence space without reference to score information that may provide critical cues to explain the observed data. To bridge this gap, we present IMMA-Emo, an integrated software system for visualising emotion data aligned with music audio and score, so as to provide an intuitive way to interactively visualise and analyse music emotion data. The visual interface also allows for the comparison of multiple emotion time series. The IMMA-Emo system builds on the online interactive Multi-modal Music Analysis (IMMA) system. Two examples demonstrating the capabilities of the IMMA-Emo system are drawn from an experiment set up to collect arousal-valence ratings based on participants' perceived emotions during a live performance. Direct observation of corresponding score parts and aural input from the recording allow explanatory factors to be identified for the ratings and changes in the ratings.},
  day = {23},
  doi = {10.1145/3123514.3123545},
  isbn = {9781450353731},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="herremans2017asystems"></a><pre>
@article{<a href="pubs2017_raw.html#herremans2017asystems">herremans2017asystems</a>,
  title = {A Functional Taxonomy of Music Generation Systems},
  author = {Herremans, D and Chuan, C-H and Chew, E},
  journal = {ACM COMPUTING SURVEYS},
  year = {2017},
  month = {Nov},
  number = {ARTN 69},
  volume = {50},
  doi = {10.1145/3108242},
  eissn = {1557-7341},
  issn = {0360-0300},
  issue = {5},
  keyword = {Music generation},
  publicationstatus = {published},
  timestamp = {2018.02.05},
  url = {<a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&amp;SrcApp=PARTNER_APP\&amp;SrcAuth=LinksAMR\&amp;KeyUT=WOS:000418294100008\&amp;DestLinkType=FullRecord\&amp;DestApp=ALL_WOS\&amp;UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a">http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&SrcApp=PARTNER_APP\&SrcAuth=LinksAMR\&KeyUT=WOS:000418294100008\&DestLinkType=FullRecord\&DestApp=ALL_WOS\&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a</a>}
}
</pre>

<a name="wangidentifyinglearning"></a><pre>
@article{<a href="pubs2017_raw.html#wangidentifyinglearning">wangidentifyinglearning</a>,
  title = {Identifying Missing and Extra Notes in Piano Recordings Using Score-Informed Dictionary Learning},
  author = {WANG, S and Ewert, SEBASTIAN and Dixon, SIMON},
  journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing},
  year = {2017},
  doi = {10.1109/TASLP.2017.2724203},
  publicationstatus = {online-published},
  timestamp = {2018.02.05}
}
</pre>

<a name="mehrabi2017vocalcentroid"></a><pre>
@article{<a href="pubs2017_raw.html#mehrabi2017vocalcentroid">mehrabi2017vocalcentroid</a>,
  title = {Vocal imitation of synthesised sounds varying in pitch, loudness and spectral centroid},
  author = {MEHRABI, A and Dixon, S and Sandler},
  journal = {Journal of the Acoustical Society of America},
  year = {2017},
  month = {Feb},
  pages = {783--796},
  volume = {141},
  abstract = {Vocal imitations are often used to convey sonic ideas [Lemaitre, Dessein, Susini, and Aura.
(2011). Ecol. Psych.
23
(4), 267–307]. For computer based systems to interpret these vocalisations,
it is advantageous to apply knowledge of what happens when people vocalise sounds where the
acoustic features have different temporal envelopes. In the present study, 19 experienced musi-
cians and music producers were asked to imitate 44 sounds with one or two feature envelopes
applied. The study addresses two main questions: (1) How accurately can people imitate ramp and
modulation envelopes for pitch, loudness, and spectral centroid?; (2) What happens to this accu-
racy when people are asked to imitate two feature envelopes simultaneously? The results show
that experienced musicians can imitate pitch, loudness, and spectral centroid accurately, and that
imitation accuracy is generally preserved when the imitated stimuli combine two, non-necessarily
congruent features. This demonstrates the viability of using the voice as a natural means of
expressing time series of two features simultaneously.},
  day = {13},
  doi = {10.1121/1.4974825},
  issn = {1520-8524},
  issue = {2},
  publicationstatus = {published},
  publisher = {Acoustical Society of America},
  timestamp = {2018.02.05},
  url = {<a href="http://asa.scitation.org/doi/abs/10.1121/1.4974825">http://asa.scitation.org/doi/abs/10.1121/1.4974825</a>}
}
</pre>

<a name="mehrabi2017musicevaluation"></a><pre>
@article{<a href="pubs2017_raw.html#mehrabi2017musicevaluation">mehrabi2017musicevaluation</a>,
  title = {Music thumbnailing for radio podcasts: A listener evaluation},
  author = {Mehrabi, A and Harte, C and Baume, C and Dixon, S},
  journal = {AES: Journal of the Audio Engineering Society},
  year = {2017},
  month = {Jun},
  pages = {474--481},
  volume = {65},
  abstract = {© 2017. When radio podcasts are produced from previously broadcast material, 30-second "thumbnails" of songs that featured in the original program are often included. Such thumbnails are made up of continuous or concatenated sections from a song and provide the audience with a summary of the music content. However, editing full-length songs down to representative thumbnails is a labor intensive process, particularly when concatenating multiple song sections. This presents an ideal application for automatic music editing tools and raises the question of how a piece of music is best summarized for this task. To gain insight into this problem we asked 120 listeners to rate the quality of thumbnails generated by eight methods (five automatic and three manual). When asked to judge overall editing quality (on a five point Likert scale) listeners gave higher ratings to methods where the edit points were quantized to bar positions, although we found no preference for structural content such as the chorus. Ratings for two automatic editing methods (one containing the chorus, one containing only the intro and outro) were not significantly different to their manual counterparts. This result suggests that the automatic editing methods applied here can be used to create production quality thumbnails.},
  day = {1},
  doi = {10.17743/jaes.2017.0011},
  issn = {1549-4950},
  issue = {6},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="panteli2017towardsmusic"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#panteli2017towardsmusic">panteli2017towardsmusic</a>,
  title = {Towards the characterization of singing styles in world music},
  author = {Panteli, M and Bittner, R and Bello, JP and Dixon, S},
  booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  year = {2017},
  month = {Jun},
  pages = {636--640},
  abstract = {© 2017 IEEE. In this paper we focus on the characterization of singing styles in world music. We develop a set of contour features capturing pitch structure and melodic embellishments. Using these features we train a binary classifier to distinguish vocal from non-vocal contours and learn a dictionary of singing style elements. Each contour is mapped to the dictionary elements and each recording is summarized as the histogram of its contour mappings. We use K-means clustering on the recording representations as a proxy for singing style similarity. We observe clusters distinguished by characteristic uses of singing techniques such as vibrato and melisma. Recordings that are clustered together are often from neighbouring countries or exhibit aspects of language and cultural proximity. Studying singing particularities in this comparative manner can contribute to understanding the interaction and exchange between world music styles.},
  conference = {IEEE International Conference on Acoustics, Speech and Signal Processing},
  day = {16},
  doi = {10.1109/ICASSP.2017.7952233},
  isbn = {9781509041176},
  issn = {1520-6149},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="nakamura2017notefields"></a><pre>
@article{<a href="pubs2017_raw.html#nakamura2017notefields">nakamura2017notefields</a>,
  title = {Note Value Recognition for Piano Transcription Using Markov Random Fields},
  author = {NAKAMURA, E and Yoshii, K and Dixon, S},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year = {2017},
  month = {Jun},
  pages = {1--1},
  abstract = {This paper presents a statistical method for use in music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times (or note values) and thus could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results show that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.},
  day = {30},
  doi = {10.1109/TASLP.2017.2722103},
  eissn = {2329-9304},
  issn = {2329-9290},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="digiorgi2017acomplexity"></a><pre>
@article{<a href="pubs2017_raw.html#digiorgi2017acomplexity">digiorgi2017acomplexity</a>,
  title = {A data-driven model of tonal chord sequence complexity},
  author = {Di Giorgi, B and Dixon, S and Zanoni, M and Sarti, A},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year = {2017},
  month = {Sep},
  pages = {1--1},
  day = {28},
  doi = {10.1109/TASLP.2017.2756443},
  eissn = {2329-9304},
  issn = {2329-9290},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="mohamad2017pickupautocorrelation"></a><pre>
@article{<a href="pubs2017_raw.html#mohamad2017pickupautocorrelation">mohamad2017pickupautocorrelation</a>,
  title = {Pickup position and plucking point estimation on an electric guitar via autocorrelation.},
  author = {Mohamad, Z and Dixon, S and Harte, C},
  journal = {J Acoust Soc Am},
  year = {2017},
  month = {Dec},
  pages = {3530--3540},
  volume = {142},
  abstract = {This paper proposes a technique that estimates the locations along the string of the plucking event and the magnetic pickup of an electric guitar based on the autocorrelation of the spectral peaks. To improve accuracy, a method is introduced to flatten the spectrum before applying the autocorrelation function to the spectral peaks. The minimum mean squared error between the autocorrelation of the observed data and the electric guitar model is found in order to estimate the model parameters. The accuracy of the algorithm is tested on various plucking positions on all open strings for each pickup configuration. The accuracy of the proposed method for various plucking dynamics and fret positions is also evaluated. The method yields accurate results: the average absolute errors of the pickup position and plucking point estimates for single pickups are 3.53 and 5.11 mm, respectively, and for mixed pickups are 8.47 and 9.95 mm, respectively. The model can reliably distinguish which pickup configuration is selected using the pickup position estimates. Moreover, the method is robust to changes in plucking dynamics and fret positions.},
  doi = {10.1121/1.5016815},
  eissn = {1520-8524},
  issue = {6},
  language = {eng},
  publicationstatus = {published},
  timestamp = {2018.02.05},
  url = {https://www.ncbi.nlm.nih.gov/pubmed/29289089}
}
</pre>

<a name="mohamad2017pickupaudioeffects"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#mohamad2017pickupaudioeffects">mohamad2017pickupaudioeffects</a>,
  title = {Estimating pickup and plucking positions of guitar tones and chords with audio effects},
  author = {Mohamad, Z and Dixon, S and Harte, C},
  booktitle = {Proceedings of the International Conference on Digital Audio Effects (DAFx)},
  year = {2017},
  pages = {420--426}
}
</pre>

<a name="mohamad2017pickupestimation"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#mohamad2017pickupestimation">mohamad2017pickupestimation</a>,
  title = {Pickup position and plucking point estimation on an electric guitar},
  author = {Mohamad, Z and Dixon, S and Harte, C},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year = {2017},
  pages = {651--655}
}
</pre>

<a name="marengo2017thewebsite"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#marengo2017thewebsite">marengo2017thewebsite</a>,
  title = {The Interaction of Casual Users with Digital Collections of Visual Art: An Exploratory Study of the WikiArt Website},
  author = {Marengo, L and FAZEKAS, G and TOMBROS, A},
  booktitle = {19th International Conference on Human-Computer Interaction (HCI’17), 9-14 July, Vancouver, Canada},
  year = {2017},
  month = {Jul},
  note = {date-added: 2017-12-22 18:06:25 +0000
date-modified: 2017-12-22 18:42:08 +0000
keywords: information retrieval, information seeking, casual interaction, curiosity, engagement
publisher-url: https://link.springer.com/chapter/10.1007\%2F978-3-319-58753-0_82
bdsk-url-1: http://www.semanticaudio.net/files/papers/marengo2017hci.pdf
bdsk-url-2: https://dx.doi.org/10.1007/978-3-319-58753-0_82},
  organization = {Vancouver, Canada},
  publisher = {Springer, Cham},
  series = {Communications in Computer and Information Science},
  volume = {714},
  abstract = {As many cultural institutions are publishing digital heritage material on the web, a new type of user emerged, that casually interacts with the art collection in his/her free time, driven by intrinsic curiosity more than by a professional duty or an informational goal. Can choices in how the interaction with data is structured increase engagement of such users? In our exploratory study, we use the WikiArt project as a case study to analyse how users approach search interfaces for free exploration. Our preliminary results show that, despite the remarkable diversity of artworks available, users rely on familiarity as their main criterion to navigate the website; they stay within known topics and rarely discover new ones. Users show interest in heterogeneous datasets, but their engagement is rarely sustained, while the presence of slightly unrelated artworks in a set can increase curiosity and self-reflection. Finally, we discuss the role of the database’s perceived size on users’ expectations.},
  conference = {19th International Conference on Human-Computer Interaction (HCI’17)},
  day = {9},
  doi = {10.1007/978-3-319-58753-0_82},
  finishday = {14},
  finishmonth = {Jul},
  finishyear = {2017},
  publicationstatus = {published},
  startday = {9},
  startmonth = {Jul},
  startyear = {2017},
  timestamp = {2018.02.05}
}
</pre>

<a name="bechhofer2017linkedanalyses"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#bechhofer2017linkedanalyses">bechhofer2017linkedanalyses</a>,
  title = {Linked Data Publication of Live Music Archives and Analyses},
  author = {Bechhofer, S and Page, K and Weigl, D and FAZEKAS, G and Wilmering, T},
  booktitle = {The Semantic Web, proc. of the 16th International Semantic Web Conference (ISWC), Oct. 21-25, Vienna, Austria},
  year = {2017},
  month = {Oct},
  note = {date-added: 2017-12-22 15:39:21 +0000
date-modified: 2017-12-22 15:53:18 +0000
keywords: Linked Data, Semantic Audio, Semantic Web, live music archive
local-url: https://link.springer.com/chapter/10.1007/978-3-319-68204-4_3
bdsk-url-1: https://iswc2017.semanticweb.org/wp-content/uploads/papers/MainProceedings/221.pdf
bdsk-url-2: https://dx.doi.org/10.1007/978-3-319-68204-4_3},
  publisher = {Springer, Cham},
  series = {Lecture Notes in Computer Science},
  volume = {10588},
  abstract = {We describe the publication of a linked data set exposing metadata from the Internet Archive Live Music Archive along with detailed feature analysis data of the audio files contained in the archive. The collection is linked to existing musical and geographical resources allowing for the extraction of useful or nteresting subsets of data using additional metadata. The collection is published using a ‘layered’ approach, aggregating the original information with links and specialised analyses, and forms a valuable resource for those investigating or developing audio analysis tools and workflows.},
  conference = {16th International Semantic Web Conference},
  day = {21},
  doi = {10.1007/978-3-319-68204-4_3},
  publicationstatus = {published},
  timestamp = {2018.02.05},
  url = {https://iswc2017.semanticweb.org/wp-content/uploads/papers/MainProceedings/221.pdf}
}
</pre>

<a name="healeyatuition"></a><pre>
@article{<a href="pubs2017_raw.html#healeyatuition">healeyatuition</a>,
  title = {A New Medium for Remote Music Tuition},
  author = {HEALEY, PGT and Duffy},
  journal = {Journal of Music, Technology and Education},
  year = {2017},
  volume = {10},
  doi = {10.1386/jmte.10.1.5_1},
  issn = {1752-7066},
  issue = {1},
  publicationstatus = {published},
  publisher = {Intellect},
  timestamp = {2018.02.05}
}
</pre>

<a name="armitage2017themicroscope"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#armitage2017themicroscope">armitage2017themicroscope</a>,
  title = {"The finer the musician, the smaller the details": NIMEcraft under the microscope},
  author = {Armitage, J and Morreale, F and McPherson, A},
  booktitle = {<a href="http://www.nime.org/archives/">http://www.nime.org/archives/</a>},
  year = {2017},
  month = {May},
  organization = {Copenhagen, Denmark},
  publisher = {New Instruments for Musical Expression},
  abstract = {Many digital musical instrument design frameworks have been proposed that are well suited for analysis and comparison. However, not all provide applicable design suggestions, especially where subtle, important details are concerned. Using traditional lutherie as a model, we conducted a series of interviews to explore how violin makers “go beyond the obvious”, and how players perceive and describe subtle details of instrumental quality. We find that lutherie frameworks provide clear design methods, but are not enough to make a fine violin. Success comes after acquiring sufficient tacit knowledge, which enables detailed craft through subjective, empirical methods. Testing instruments for subtle qualities was suggested to be a different skill to playing. Whilst players are able to identify some specific details about instrumental quality by comparison, these are often not actionable, and important aspects of “sound and feeling” are much more difficult to describe. In the DMI domain, we introduce the term NIMEcraft to describe subtle differences between otherwise identical instruments and their underlying design processes, and consider how to improve the dissemination of NIMEcraft.},
  conference = {New Instruments for Musical Expression 2017},
  day = {14},
  finishday = {19},
  finishmonth = {May},
  finishyear = {2017},
  keyword = {craft},
  publicationstatus = {online-published},
  startday = {14},
  startmonth = {May},
  startyear = {2017},
  timestamp = {2018.02.05},
  url = {<a href="http://www.instrumentslab.org/">http://www.instrumentslab.org/</a>}
}
</pre>

<a name="harrison2017adaptingplaying"></a><pre>
@article{<a href="pubs2017_raw.html#harrison2017adaptingplaying">harrison2017adaptingplaying</a>,
  title = {Adapting the Bass Guitar for One-Handed Playing},
  author = {HARRISON, JTF and McPherson, AP},
  journal = {Journal of New Music Research},
  year = {2017},
  month = {Jun},
  abstract = {This paper presents a prototype system for adapting the bass guitar for one-handed musicians. We discuss existing solutions to accessible musical instruments, followed by the results of an online survey of bass guitarists, which informed the design of a prototype bass guitar adaptation. The adaptation comprises a foot-operated MIDI controller with a solenoid-actuated fretting mechanism, providing access to six frets across two strings of the bass. A study involving six bassists rehearsing and writing a bass guitar accompaniment with the adapted bass highlighted unexpected facets of bass guitar playing, and provided insights into the design of future accessible string instruments.},
  day = {28},
  doi = {10.1080/09298215.2017.1340485},
  issn = {0929-8215},
  publicationstatus = {published},
  publisher = {Taylor \& Francis (Routledge)},
  timestamp = {2018.02.05}
}
</pre>

<a name="pearce2017compressionbasedperception"></a><pre>
@article{<a href="pubs2017_raw.html#pearce2017compressionbasedperception">pearce2017compressionbasedperception</a>,
  title = {Compression-based Modelling of Musical Similarity Perception},
  author = {Pearce, M and Müllensiefen, D},
  journal = {Journal of New Music Research},
  year = {2017},
  month = {Apr},
  note = {peerreview_statement: The publishing and review policy for this title is described in its Aims \& Scope.
aims_and_scope_url: http://www.tandfonline.com/action/journalInformation?show=aimsScope\&journalCode=nnmr20},
  pages = {135--155},
  volume = {46},
  day = {3},
  doi = {10.1080/09298215.2017.1305419},
  eissn = {1744-5027},
  issn = {0929-8215},
  issue = {2},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="vanderweij2017aenculturation"></a><pre>
@article{<a href="pubs2017_raw.html#vanderweij2017aenculturation">vanderweij2017aenculturation</a>,
  title = {A Probabilistic Model of Meter Perception: Simulating Enculturation.},
  author = {van der Weij, B and Pearce, MT and Honing, H},
  journal = {Front Psychol},
  year = {2017},
  month = {May},
  pages = {824--824},
  volume = {8},
  abstract = {Enculturation is known to shape the perception of meter in music but this is not explicitly accounted for by current cognitive models of meter perception. We hypothesize that the induction of meter is a result of predictive coding: interpreting onsets in a rhythm relative to a periodic meter facilitates prediction of future onsets. Such prediction, we hypothesize, is based on previous exposure to rhythms. As such, predictive coding provides a possible explanation for the way meter perception is shaped by the cultural environment. Based on this hypothesis, we present a probabilistic model of meter perception that uses statistical properties of the relation between rhythm and meter to infer meter from quantized rhythms. We show that our model can successfully predict annotated time signatures from quantized rhythmic patterns derived from folk melodies. Furthermore, we show that by inferring meter, our model improves prediction of the onsets of future events compared to a similar probabilistic model that does not infer meter. Finally, as a proof of concept, we demonstrate how our model can be used in a simulation of enculturation. From the results of this simulation, we derive a class of rhythms that are likely to be interpreted differently by enculturated listeners with different histories of exposure to rhythms.},
  day = {22},
  doi = {10.3389/fpsyg.2017.00824},
  keyword = {cognition},
  language = {eng},
  publicationstatus = {online-published},
  timestamp = {2018.02.05},
  url = {https://www.ncbi.nlm.nih.gov/pubmed/28588533}
}
</pre>

<a name="cameron2017perceptioncontext"></a><pre>
@article{<a href="pubs2017_raw.html#cameron2017perceptioncontext">cameron2017perceptioncontext</a>,
  title = {Perception of Rhythmic Similarity is Asymmetrical, and Is Influenced by Musical Training, Expressive Performance, and Musical Context},
  author = {Cameron, D and Potter, K and Wiggins, G and PEARCE, MT},
  journal = {Timing and Time Perception},
  year = {2017},
  month = {Jun},
  abstract = {Rhythm is an essential part of the structure, behaviour, and aesthetics of music. However, the cog- nitive processing that underlies the perception of musical rhythm is not fully understood. In this study, we tested whether rhythm perception is influenced by three factors: musical training, the presence of expressive performance cues in human-performed music, and the broader musical con- text. We compared musicians and nonmusicians’ similarity ratings for pairs of rhythms taken from Steve Reich’s Clapping Music. The rhythms were heard both in isolation and in musical context and both with and without expressive performance cues. The results revealed that rhythm perception is influenced by the experimental conditions: rhythms heard in musical context were rated as less similar than those heard in isolation; musicians’ ratings were unaffected by expressive performance, but nonmusicians rated expressively performed rhythms as less similar than those with exact timing; and expressively-performed rhythms were rated as less similar compared to rhythms with exact timing when heard in isolation but not when heard in musical context. The results also showed asymmetrical perception: the order in which two rhythms were heard influenced their perceived similarity. Analyses suggest that this asymmetry was driven by the internal coherence of rhythms, as measured by normalized Pairwise Variability Index (nPVI). As predicted, rhythms were perceived as less similar when the first rhythm in a pair had greater coherence (lower nPVI) than the second rhythm, compared to when the rhythms were heard in the opposite order.},
  day = {1},
  doi = {10.1163/22134468-00002085},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="sears2017simulatingmodels"></a><pre>
@article{<a href="pubs2017_raw.html#sears2017simulatingmodels">sears2017simulatingmodels</a>,
  title = {Simulating melodic and harmonic expectations for tonal cadences using probabilistic models},
  author = {Sears, DRW and PEARCE, MT and Caplin, WE and McAdams, S},
  journal = {Journal of New Music Research},
  year = {2017},
  month = {Sep},
  day = {8},
  doi = {10.1080/09298215.2017.1367010},
  issn = {0929-8215},
  publicationstatus = {published},
  publisher = {Taylor \& Francis (Routledge)},
  timestamp = {2018.02.05}
}
</pre>

<a name="chourdakis2017areverberation"></a><pre>
@article{<a href="pubs2017_raw.html#chourdakis2017areverberation">chourdakis2017areverberation</a>,
  title = {A Machine-Learning Approach to Application of Intelligent Artificial Reverberation},
  author = {Chourdakis, E and REISS, J},
  journal = {Journal of the Audio Engineering Society},
  year = {2017},
  month = {Feb},
  volume = {65},
  abstract = {We propose a design of an adaptive digital audio effect for artificial reverberation, controlled
directly by desired reverberation characteristics, that allows it to learn from the user in a super-
vised way. The user provides monophonic examples of desired reverberation characteristics
for individual tracks taken from the Open Multitrack Testbed. We use this data to train a set of
models to automatically apply reverberation to similar tracks. We evaluate those models using
classifier f1-scores, mean squared errors, and multi-stimulus listening tests.},
  day = {16},
  issn = {1549-4950},
  publicationstatus = {published},
  publisher = {Audio Engineering Society},
  timestamp = {2018.02.05}
}
</pre>

<a name="zhu2017practicalprocesses"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#zhu2017practicalprocesses">zhu2017practicalprocesses</a>,
  title = {Practical considerations on optimising multistage decimation and interpolation processes},
  author = {Zhu, X and Wang, Y and Hu, W and Reiss, JD},
  booktitle = {International Conference on Digital Signal Processing, DSP},
  year = {2017},
  month = {Mar},
  pages = {370--374},
  abstract = {© 2016 IEEE. Multistage filter design is a complex multidimensional optimisation problem. The formulae for optimal design generally yield non-integer real numbers for the sample-rate-changing factors of multiple stages. Approaches yielding useful integer results have high computational cost and do not consider important multistage filter design properties. We have developed a simplified algorithm for directly searching the optimal integer results. Considering the most useful practical design parameters, optimal results can be approximated with a limited number of sets for any designs satisfying certain constraints, with negligible costs. This vastly simplifies the complexity of the problem.},
  day = {1},
  doi = {10.1109/ICDSP.2016.7868581},
  isbn = {9781509041657},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="selfridge2017realtimesounds"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#selfridge2017realtimesounds">selfridge2017realtimesounds</a>,
  title = {REAL-TIME PHYSICAL MODEL FOR SYNTHESIS OF SWORD SWING SOUNDS},
  author = {SELFRIDGE, R and Moffat, D and Reiss, J},
  year = {2017},
  month = {Jul},
  organization = {Espoo, Finland},
  abstract = {Sword sounds are synthesised by physical models in real- time. A number of compact sound sources are used along the length of the sword which replicate the swoosh sound when swung through the air. Listening tests are carried out which reveal a model with reduced physics is perceived as more authentic. The model is further developed to be controlled by a Wii Controller and successfully extended to include sounds of a baseball bat and golf club.},
  conference = {14th Sound and Music Computing Conference},
  day = {8},
  finishday = {8},
  finishmonth = {Jul},
  finishyear = {2017},
  publicationstatus = {published},
  startday = {5},
  startmonth = {Jul},
  startyear = {2017},
  timestamp = {2018.02.05}
}
</pre>

<a name="song2017performancetypes"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#song2017performancetypes">song2017performancetypes</a>,
  title = {Performance evaluation of a new flexible time division multiplexing protocol on mixed traffic types},
  author = {Song, Y and Wang, Y and Bull, P and Reiss, JD},
  booktitle = {Proceedings - International Conference on Advanced Information Networking and Applications, AINA},
  year = {2017},
  month = {May},
  pages = {23--30},
  abstract = {© 2017 IEEE. The broadcasting industry has recently begun to adopt statistical multiplexing based network platform in their workflow to support professional live audio/video (AV) transmission instead of the Time Division Multiplexing (TDM) based system. These audio-over-packet switched systems require a carefully designed and managed network to ensure key quality measures of the real-time (RT) media, such as low jitter and low latency. Often the best effort traffic or different types of media are still physically or logically segregated from these dedicated systems, or require large redundant links. The proposed Flexilink architecture is an alternative that combines both circuit switched and best effort features. However, there is no research evaluation that shows the actual performance of this proposed architecture. In this paper, we give a simulation based study and critical evaluation of the performance of the Flexilink network. The simulation results show that Flexilink has a better and more stable RT performance when compared with both Ethernet and priority queueing networks, especially when given a burst of traffic and/or multiple RT traffic sources. In addition, Unlike other networking protocols, jitter in Flexilink is below the audible threshold.},
  conference = {2017 IEEE 31st International Conference on Advanced Information Networking and Applications},
  day = {5},
  doi = {10.1109/AINA.2017.80},
  isbn = {9781509060283},
  issn = {1550-445X},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="selfridge2017realtimeharp"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#selfridge2017realtimeharp">selfridge2017realtimeharp</a>,
  title = {Real-time physical model of an Aeolian harp},
  author = {SELFRIDGE, R and Moffat, D and Reiss, J and Avital, E},
  year = {2017},
  month = {Jul},
  organization = {London, UK},
  abstract = {A real-time physical sound synthesis model of an Aeolian harp is presented. The model uses semi- empirical fluid dynamics equations to inform its operation, providing suitable parameters for users to interact. A basic wind model is included as well as an interface allowing user adjustable param- eters. Sounds generated by the model were subject to objective measurements against real-world recordings, which showed that many of the physical properties of the harp were replicated in our model, but a possible link between harmonics and vibration amplitude was not. A perceptual test was performed, where participants were asked to rate sounds in terms of how plausible they were in comparison with spectral modelling synthesis and recorded Aeolian Harp samples. Evaluation showed that our model performed as well as an alternative non-physical synthesis method, but was not as authentic as actual recorded samples.},
  conference = {24th International Congress on Sound and Vibration},
  day = {27},
  finishday = {27},
  finishmonth = {Jul},
  finishyear = {2017},
  publicationstatus = {published},
  startday = {24},
  startmonth = {Jul},
  startyear = {2017},
  timestamp = {2018.02.05}
}
</pre>

<a name="milo2017auralmap"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#milo2017auralmap">milo2017auralmap</a>,
  title = {Aural Fabric: An interactive textile sonic map},
  author = {Milo, A and Reiss, JD},
  booktitle = {ACM International Conference Proceeding Series},
  year = {2017},
  month = {Aug},
  volume = {Part F131930},
  abstract = {© 2017 Copyright held by the owner/author(s). The Aural Fabric is an interactive textile sonic map created to promote engagement in acoustic awareness towards the built environment. It fosters discussions on the aural environment of our cities by allowing users to experience binaural recordings captured during a soundwalk. The touch of the conductive areas embroidered of the surface on the map can be sensed by two capacitive boards stitched on the map. These are externally connected to an embedded computer processing unit, Bela. The recordings can be intuitively mixed together offering exploratory and performative recall of the material collected.},
  day = {23},
  doi = {10.1145/3123514.3123565},
  isbn = {9781450353731},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="selfridge2017physicallypropeller"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#selfridge2017physicallypropeller">selfridge2017physicallypropeller</a>,
  title = {Physically derived sound synthesis model of a propeller},
  author = {Selfridge, R and Moffat, D and Reiss, JD},
  booktitle = {ACM International Conference Proceeding Series},
  year = {2017},
  month = {Aug},
  volume = {Part F131930},
  abstract = {© 2017 Copyright held by the owner/author(s). A real-time sound synthesis model for propeller sounds is presented. Equations obtained from fluid dynamics and aerodynamics research are utilised to produce authentic propeller-powered aircraft sounds. The result is a physical model in which the geometries of the objects involved are used in sound synthesis calculations. The model operates in real-time making it ideal for integration within a game or virtual reality environment. Comparison with real propeller-powered aircraft sounds indicates that some aspects of real recordings are not replicated by our model. Listening tests suggest that our model performs as well as another synthesis method but is not as plausible as a real recording.},
  day = {23},
  doi = {10.1145/3123514.3123524},
  isbn = {9781450353731},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="chourdakis2017constructingpolicies"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#chourdakis2017constructingpolicies">chourdakis2017constructingpolicies</a>,
  title = {Constructing narrative using a generative model and continuous action policies},
  author = {CHOURDAKIS, ET and REISS, JD},
  booktitle = {<a href="http://aclanthology.info/events/ws-2017">http://aclanthology.info/events/ws-2017</a>},
  year = {2017},
  month = sep,
  organization = {University of Santiago De Compostela},
  abstract = {This paper proposes a method for learning
how to generate narrative by recombining sentences
from a previous collection. Given a corpus
of story events categorised into 9 topics,
we approximate a deep reinforcement learning
agent policy to recombine them in order
to satisfy narrative structure. We also propose
an evaluation of such a system. The evaluation
is based on coherence, interest, and topic,
in order to figure how much sense the generated
stories make, how interesting they are,
and examine whether new narrative topics can
emerge.},
  conference = {Workshop on Computational Creativity in Natural Language Generation},
  day = {4},
  finishday = {4},
  finishmonth = {Sep},
  finishyear = {2017},
  keyword = {story generation},
  publicationstatus = {published},
  startday = {4},
  startmonth = {Sep},
  startyear = {2017},
  timestamp = {2018.02.05}
}
</pre>

<a name="wilkinson2017latentrecordings"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#wilkinson2017latentrecordings">wilkinson2017latentrecordings</a>,
  title = {Latent force models for sound: Learning modal synthesis parameters and excitation functions from audio recordings},
  author = {Wilkinson, WJ and Reiss, JD and Stowell, D},
  booktitle = {DAFx 2017 - Proceedings of the 20th International Conference on Digital Audio Effects},
  year = {2017},
  month = {Oct},
  pages = {56--63},
  abstract = {Latent force models are a Bayesian learning technique that combine physical knowledge with dimensionality reduction - sets of coupled differential equations are modelled via shared dependence on a low-dimensional latent space. Analogously, modal sound synthesis is a technique that links physical knowledge about the vibration of objects to acoustic phenomena that can be observed in data. We apply latent force modelling to sinusoidal models of audio recordings, simultaneously inferring modal synthesis parameters (stiffness and damping) and the excitation or contact force required to reproduce the behaviour of the observed vibrational modes. Exposing this latent excitation function to the user constitutes a controllable synthesis method that runs in real time and enables sound morphing through interpolation of learnt parameters.},
  conference = {Proceedings of the 20th International Conference on Digital Audio Effects (DAFx-17)},
  day = {5},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="moffat2017unsupervisedeffects"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#moffat2017unsupervisedeffects">moffat2017unsupervisedeffects</a>,
  title = {Unsupervised Taxonomy of Sound Effects},
  author = {MOFFAT, DJ and Ronan, D and Reiss, JD},
  year = {2017},
  month = {Sep},
  organization = {Edinburgh, UK},
  conference = {20th International Conference on Digital Audio Effects (DAFx-17),},
  day = {5},
  finishday = {8},
  finishmonth = {Sep},
  finishyear = {2017},
  publicationstatus = {published},
  startday = {5},
  startmonth = {Sep},
  startyear = {2017},
  timestamp = {2018.02.05}
}
</pre>

<a name="selfridge2017soundmodels"></a><pre>
@article{<a href="pubs2017_raw.html#selfridge2017soundmodels">selfridge2017soundmodels</a>,
  title = {Sound synthesis of objects swinging through air using physical models},
  author = {Selfridge, R and Moffat, D and Reiss, JD},
  journal = {Applied Sciences (Switzerland)},
  year = {2017},
  month = {Nov},
  volume = {7},
  abstract = {© 2017 by the authors. A real-time physically-derived sound synthesis model is presented that replicates the sounds generated as an object swings through the air. Equations obtained from fluid dynamics are used to determine the sounds generated while exposing practical parameters for a user or game engine to vary. Listening tests reveal that for the majority of objects modelled, participants rated the sounds from our model as plausible as actual recordings. The sword sound effect performed worse than others, and it is speculated that one cause may be linked to the difference between expectations of a sound and the actual sound for a given object.},
  day = {16},
  doi = {10.3390/app7111177},
  eissn = {2076-3417},
  issue = {11},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="morodynamicsignificance"></a><pre>
@article{<a href="pubs2017_raw.html#morodynamicsignificance">morodynamicsignificance</a>,
  title = {Dynamic behaviour of the keyboard action on the Hammond organ and its perceptual significance},
  author = {MORO, G and MCPHERSON, A and SANDLER, M},
  journal = {Journal of the Acoustical Society of America},
  year = {2017},
  issn = {1520-8524},
  publicationstatus = {accepted},
  publisher = {Acoustical Society of America},
  timestamp = {2018.02.05}
}
</pre>

<a name="fanoyelainterferencefactorization"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#fanoyelainterferencefactorization">fanoyelainterferencefactorization</a>,
  title = {INTERFERENCE REDUCTION IN MUSIC RECORDINGS COMBINING KERNEL ADDITIVE MODELLING AND NON-NEGATIVE MATRIX FACTORIZATION},
  author = {FANO YELA, D and Ewert, SE and Sandler, MS and FitzGerald, DF},
  year = {2017},
  organization = {New Orleans},
  abstract = {In live and studio recordings unexpected sound events often lead to interferences in the signal. For non-stationary interferences, sound source separation techniques can be used to reduce the interference level in the recording. In this context, we present a novel approach combining the strengths of two algorithmic families: NMF and KAM. The recent KAM approach applies robust statistics on frames selected by a source-specific kernel to perform source separation. Based on semi-supervised NMF, we extend this approach in two ways. First, we locate the interference in the recording based on detected NMF activity. Second, we improve the kernel-based frame selection by incorporating an NMF-based estimate of the clean music signal. Further, we introduce a temporal context in the kernel, taking some musical structure into account. Our experiments show improved separation quality for our proposed method over a state-of-the-art approach for interference reduction.},
  conference = {IEEE International Conference on Acoustics, Speech and Signal Processings},
  finishday = {9},
  finishmonth = {Mar},
  finishyear = {2017},
  keyword = {Source Separation},
  publicationstatus = {published},
  startday = {5},
  startmonth = {Mar},
  startyear = {2017},
  timestamp = {2018.02.05}
}
</pre>

<a name="ewert2017structuredseparation"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#ewert2017structuredseparation">ewert2017structuredseparation</a>,
  title = {Structured dropout for weak label and multi-instance learning and its application to score-informed source separation},
  author = {Ewert, S and Sandler, MB},
  booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  year = {2017},
  month = {Jun},
  pages = {2277--2281},
  abstract = {© 2017 IEEE. Many success stories involving deep neural networks are instances of supervised learning, where available labels power gradient-based learning methods. Creating such labels, however, can be expensive and thus there is increasing interest in weak labels which only provide coarse information, with uncertainty regarding time, location or value. Using such labels often leads to considerable challenges for the learning process. Current methods for weak-label training often employ standard supervised approaches that additionally reassign or prune labels during the learning process. The information gain, however, is often limited as only the importance of labels where the network already yields reasonable results is boosted. We propose treating weak-label training as an unsupervised problem and use the labels to guide the representation learning to induce structure. To this end, we propose two autoencoder extensions: class activity penalties and structured dropout. We demonstrate the capabilities of our approach in the context of score-informed source separation of music.},
  conference = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  day = {16},
  doi = {10.1109/ICASSP.2017.7952562},
  isbn = {9781509041176},
  issn = {1520-6149},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="ohanlon2017improvedfeature"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#ohanlon2017improvedfeature">ohanlon2017improvedfeature</a>,
  title = {Improved template-based chord recognition using the CRP feature},
  author = {O'Hanlon, KO and Ewert, S and Pauwels, J and Sandler, M},
  year = {2017},
  month = {Jun},
  conference = {2017 IEEE Conference on Acoustics, Speech and Signal Processing},
  day = {19},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="quinton2017trackingsparsenmf"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#quinton2017trackingsparsenmf">quinton2017trackingsparsenmf</a>,
  title = {TRACKING METRICAL STRUCTURE CHANGES WITH SPARSE-NMF},
  author = {Quinton, E and Dixon, S and Sandler, M and O'Hanlon, KO},
  year = {2017},
  month = {Jun},
  conference = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing},
  day = {19},
  publicationstatus = {accepted},
  timestamp = {2018.02.05}
}
</pre>

<a name="white2017anvocalisation"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#white2017anvocalisation">white2017anvocalisation</a>,
  title = {An Archival Echo: Recalling the public domain through real-time query by vocalisation},
  author = {White, B and Mehrabi, A and Sandler, MB},
  booktitle = {ACM International Conference Proceeding Series},
  year = {2017},
  month = {Aug},
  volume = {Part F131930},
  abstract = {© 2017 Copyright held by the owner/author(s). In this paper we present a novel system for performative interaction with an archive of public domain music recordings. The system uses real-time query by vocalization to retrieve sounds extracted from chart hit singles of the 1960s. This enables the user, or performer, to generate a cascade of archival echoes from vocalisations. The system was developed for a series of music workshops, held as part of a two art projects centered around the reuse and repurposing of archive recordings. As such the design decisions were shaped by the conceptual framework of the artists and intended audience. Here we outline the context and background of the art projects, describe the query by vocalisation system and discuss the workshops, where the artists invited amateur musicians to use the system to develop a public performance.},
  day = {23},
  doi = {10.1145/3123514.3123546},
  isbn = {9781450353731},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="mcarthur2017distanceopportunities"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#mcarthur2017distanceopportunities">mcarthur2017distanceopportunities</a>,
  title = {Distance in audio for VR: Constraints and opportunities},
  author = {McArthur, A and Sandler, M and Stewart, R},
  booktitle = {ACM International Conference Proceeding Series},
  year = {2017},
  month = {Aug},
  volume = {Part F131930},
  abstract = {© 2017 Copyright is held by the owner/author(s). Spatial audio is enjoying a surge in attention in both scene and object based paradigms, due to the trend for, and accessibility of, immersive experience. This has been enabled through convergence in computing enhancements, component size reduction, and associated price reductions. For the first time, applications such as virtual reality (VR) are technologies for the consumer. Audio for VR is captured to provide a counterpart to the video or animated image, and can be rendered to combine elements of physical and psychoacoustic modelling, as well as artistic design. Given that distance is an inherent property of spatial audio, that it can augment sound's efficacy in cueing user attention (a problem which practitioners are seeking to solve), and that conventional film sound practices have intentionally exploited its use, the absence of research on its implementation and effects in immersive environments is notable. This paper sets out the case for its importance, from a perspective of research and practice. It focuses on cinematic VR, whose challenges for spatialized audio are clear, and at times stretches beyond the restrictions specific to distance in audio for VR, into more general audio constraints.},
  day = {23},
  doi = {10.1145/3123514.3123530},
  isbn = {9781450353731},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="choi2017transfertasks"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#choi2017transfertasks">choi2017transfertasks</a>,
  title = {Transfer learning for music classification and regression tasks},
  author = {CHOI, K and FAZEKAS, G and SANDLER, M and Kyunghyun, C},
  year = {2017},
  month = {Oct},
  conference = {The 18th International Society of Music Information Retrieval (ISMIR) Conference},
  day = {23},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="stewart2017initialsensors"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#stewart2017initialsensors">stewart2017initialsensors</a>,
  title = {Initial Investigations into Characterizing DIY E-Textile Stretch Sensors},
  author = {STEWART, RL and Skach, S},
  year = {2017},
  month = {Jun},
  organization = {London, UK},
  abstract = {is paper evaluates three electronic textile (e-textile) stretch sensors commonly constructed for bespoke applications: two variations of fabric knit with a stainless steel and polyester yarn, and knit fabric coated with a conductive polymer. Two versions of the knit stainless steel and polyester yarn sensor, one hand and one machine knit, are evaluated. All of the materials used in the construction of the sensors are accessible to designers and engineers, and are commonly used in wearable technology projects, particularly for arts performance. However, the properties of each sensor have not before been formally analysed. We evaluate the sensors’ performance when being stretched and released.},
  conference = {4th International Conference on Movement Computing},
  day = {22},
  doi = {10.1145/3077981.3078043},
  finishday = {30},
  finishmonth = {Jun},
  finishyear = {2017},
  publicationstatus = {accepted},
  startday = {28},
  startmonth = {May},
  startyear = {2017},
  timestamp = {2018.02.05}
}
</pre>

<a name="jack2017maintaininginstruments"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#jack2017maintaininginstruments">jack2017maintaininginstruments</a>,
  title = {Maintaining and constraining performer touch in the design of digital musical instruments},
  author = {Jack, RH and Stockman, T and McPherson, A},
  booktitle = {TEI 2017 - Proceedings of the 11th International Conference on Tangible, Embedded, and Embodied Interaction},
  year = {2017},
  month = {Mar},
  pages = {717--720},
  abstract = {Expression in musical practice is inextricably tied to the touch of the performer. In digital musical instruments (DMIs) the relationship of touch to sound is indirect: The nuances and fine detail of performer control can be flattened and limited during the translation of physical gesture to physical sound. The locus of this research is in the contact made between performer and DMI: focusing on this area can grant insight on fundamental issues of human computer interaction, particularly regarding intimate and expressive control of tangible interfaces. In this paper I present my research on this topic so far, which includes empirical studies that focus on specific parameters of performance where touch plays an integral role. The first study investigates how dynamic vibrations in an instrument's body can guide the hand of a performer and assist with intonation. The second study looks at asynchrony between action and sound and the influence this latency has on the perceived quality of an instrument.},
  day = {20},
  doi = {10.1145/3024969.3025042},
  isbn = {9781450346764},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="jack2017richtechnique"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#jack2017richtechnique">jack2017richtechnique</a>,
  title = {Rich gesture, reduced control: The influence of constrained mappings on performance technique},
  author = {Jack, RH and Stockman, T and McPherson, A},
  booktitle = {ACM International Conference Proceeding Series},
  year = {2017},
  month = {Jun},
  volume = {Part F129150},
  abstract = {© 2017 Copyright held by the owner/author(s). This paper presents an observational study of the interaction of professional percussionists with a simplified hand percussion instrument. We reflect on how the sound-producing gestural language of the percussionists developed over the course of an hour session, focusing on the elements of their gestural vocabulary that remained in place at the end of the session, and on those that ceased to be used. From these observations we propose a model of movement based digital musical instruments as a projection downwards from a multidimensional body language to a reduced set of sonic features or behaviours. Many factors of an instrument's design, above and beyond the mapping of sensor degrees of freedom to dimensions of control, condition the way this projection downwards happens. We argue that there exists a world of richness of gesture beyond that which the sensors capture, but which can be implicitly captured by the design of the instrument through its physicality, constituent materials and form. We provide a case study of this model in action.},
  day = {28},
  doi = {10.1145/3077981.3078039},
  isbn = {1595930361},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="deacon2017userstudy"></a><pre>
@incollection{<a href="pubs2017_raw.html#deacon2017userstudy">deacon2017userstudy</a>,
  title = {User experience in an interactive music virtual reality system: An exploratory study},
  author = {Deacon, T and Stockman, T and Barthet, M},
  publisher = {Springer},
  year = {2017},
  month = {Sep},
  pages = {192--216},
  volume = {10525 LNCS},
  abstract = {The Objects VR interface and study explores interactive music and virtual reality, focusing on user experience, understanding of musical functionality, and interaction issues. Our system offers spatio-temporal music interaction using 3D geometric shapes and their designed relationships. Control is provided by tracking of the hands, and the experience is rendered across a head-mounted display with binaural sound presented over headphones. The evaluation of the system uses a mixed methods approach based on semi-structured interviews, surveys and video-based interaction analysis. On average the system was positively received in terms of interview self-report, metrics for spatial presence and creative support. Interaction analysis and interview thematic analysis also revealed instances of frustration with interaction and levels of confusion with system functionality. Our results allow reflection on design criteria and discussion of implications for facilitating music engagement in virtual reality. Finally our work discusses the effectiveness of measures with respect to future evaluation of novel interactive music systems in virtual reality.},
  conference = {CMMR 2016},
  day = {16},
  doi = {10.1007/978-3-319-67738-5_12},
  eissn = {1611-3349},
  isbn = {9783319677378},
  issn = {0302-9743},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="sturmbringinggeneration"></a><pre>
@article{<a href="pubs2017_raw.html#sturmbringinggeneration">sturmbringinggeneration</a>,
  title = {Bringing the models back to music practice: The evaluation of deep learning approaches to music transcription modelling and generation},
  author = {STURM, BLT and Ben-Tal, O},
  journal = {Journal of Creative Music Systems},
  year = {2017},
  publicationstatus = {published},
  timestamp = {2018.02.05}
}
</pre>

<a name="liang2017detection"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#liang2017detection">liang2017detection</a>,
  title = {Detection of Piano Pedaling Techniques on the Sustain Pedal},
  author = {Liang, B and Fazekas, G and Sandler, M},
  booktitle = {<a href="http://www.aes.org/e-lib/">http://www.aes.org/e-lib/</a>},
  year = {2017},
  month = {Octobor},
  organization = {New York, USA},
  publisher = {Audio Engineering Society},
  abstract = {Automatic detection of piano pedalling techniques is challenging as it is comprised of subtle nuances of piano timbres. In this paper, we address this problem on single notes using decision-tree-based support vector machines. Features are extracted from harmonics and residuals based on physical acoustics considerations and signal observations. We consider four distinct pedalling techniques on the sustain pedal (anticipatory full, anticipatory half, legato full and legato half pedalling) and create a new isolated-note dataset consisting of different pitches and velocities for each pedalling technique plus notes played without pedal. Our results using cross-validation trails show the effectiveness of the designed features and the trained classifiers for discriminating pedalling techniques.},
  conference = {143rd Convention of the Audio Engineering Society},
  day = {18},
  finishday = {21},
  finishmonth = {Octobor},
  finishyear = {2017},
  keyword = {piano pedal},
  publicationstatus = {online-published},
  startday = {18},
  startmonth = {Octobor},
  startyear = {2017},
  timestamp = {2018.02.06},
  url = {<a href="http://www.aes.org/e-lib/browse.cfm?elib=19209">http://www.aes.org/e-lib/browse.cfm?elib=19209</a>}
}
</pre>

<a name="liang2017recognition"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#liang2017recognition">liang2017recognition</a>,
  title = {Recognition of Piano Pedalling Techniques Using Gesture Data},
  author = {Liang, B and Fazekas, G and Sandler, M},
  booktitle = {dl.acm.org},
  year = {2017},
  month = {Aug},
  organization = {Queen Mary University of London, London},
  publisher = {ACM},
  volume = {Proceedings of AM ’17},
  abstract = {This paper presents a study of piano pedalling technique recognition on the sustain pedal utilising gesture data that is collected using a novel measurement system. The recognition is comprised of two separate tasks: onset/offset detection and classification. The onset and offset time of each pedalling technique was computed through signal processing algorithms. Based on features extracted from every segment when the pedal is pressed, the task of classifying the segments by pedalling technique was undertaken using machine learning methods. We exploited and compared a Support Vector Machine (SVM) and a hidden Markov model (HMM) for classification. Recognition results can be represented by customised pedalling notations and visualised in a score following system.},
  conference = {Audio Mostly 2017},
  day = {23},
  doi = {10.1145/3123514.3123535},
  finishday = {26},
  finishmonth = {Aug},
  finishyear = {2017},
  isbn = {978-1-4503-5373-1},
  keyword = {piano pedal},
  publicationstatus = {published},
  startday = {23},
  startmonth = {Aug},
  startyear = {2017},
  timestamp = {2018.02.06},
  url = {https://dl.acm.org/citation.cfm?id=3123535}
}
</pre>

<a name="liang2017pianopedaller"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#liang2017pianopedaller">liang2017pianopedaller</a>,
  title = {Piano Pedaller: A Measurement System for Classification and Visualisation of Piano Pedalling Techniques},
  author = {Liang, B and Fazekas, G and McPherson, A and Sandler, M},
  booktitle = {<a href="http://www.nime.org/archives/">http://www.nime.org/archives/</a>},
  year = {2017},
  month = {May},
  organization = {Copenhagen, Denmark},
  publisher = {New Instruments for Musical Expression},
  abstract = {This paper presents the results of a study of piano pedalling techniques on the sustain pedal using a newly designed measurement system named Piano Pedaller. The system is comprised of an optical sensor mounted in the piano pedal bearing block and an embedded platform for recording audio and sensor data. This enables recording the pedalling gesture of real players and the piano sound under normal playing conditions. Using the gesture data collected from the system, the task of classifying these data by pedalling technique was undertaken using a Support Vector Machine (SVM). Results can be visualised in an audio based score following application to show pedalling together with the player's position in the score.},
  conference = {New Instruments for Musical Expression 2017},
  day = {14},
  finishday = {19},
  finishmonth = {May},
  finishyear = {2017},
  keyword = {piano pedal},
  publicationstatus = {online-published},
  startday = {14},
  startmonth = {May},
  startyear = {2017},
  timestamp = {2018.02.06},
  url = {<a href="http://homes.create.aau.dk/dano/nime17/papers/0062/index.html">http://homes.create.aau.dk/dano/nime17/papers/0062/index.html</a>}
}
</pre>

<a name="pigrem2017datascaping"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#pigrem2017datascaping">pigrem2017datascaping</a>,
  title = {Datascaping: Data Sonification as a Narrative Device in Soundscape Composition},
  author = {Pigrem, Jon and Barthet, Mathieu},
  booktitle = {Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences},
  pages = {43},
  year = {2017},
  organization = {ACM}
}
</pre>

<a name="ewert2017andecoding"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#ewert2017andecoding">ewert2017andecoding</a>,
  author = {Ewert, S and Sandler, MB},
  title = {An augmented lagrangian method for piano transcription using equal loudness thresholding and lstm-based decoding},
  booktitle = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
  year = {2017},
  volume = {2017-October},
  pages = {146--150},
  month = {Dec},
  abstract = {© 2017 IEEE. A central goal in automatic music transcription is to detect individual note events in music recordings. An important variant is instrument-dependent music transcription where methods can use calibration data for the instruments in use. However, despite the additional information, results rarely exceed an f-measure of 80\%. As a potential explanation, the transcription problem can be shown to be badly conditioned and thus relies on appropriate regularization. A recently proposed method employs a mixture of simple, convex regularizers (to stabilize the parameter estimation process) and more complex terms (to encourage more meaningful structure). In this paper, we present two extensions to this method. First, we integrate a computational loudness model to better differentiate real from spurious note detections. Second, we employ (Bidirectional) Long Short Term Memory networks to re-weight the likelihood of detected note constellations. Despite their simplicity, our two extensions lead to a drop of about 35\% in note error rate compared to the state-of-the-art.},
  day = {7},
  doi = {10.1109/WASPAA.2017.8170012},
  isbn = {9781538616321},
  publicationstatus = {published}
}
</pre>

<a name="bengler2017demohour"></a><pre>
@article{<a href="pubs2017_raw.html#bengler2017demohour">bengler2017demohour</a>,
  author = {Bengler, B and Martin, F and Bryan-Kinns, N and Frauenberger, C and Makhaeva, J and Spiel, K and Vishkaie, R and Jones, L},
  title = {Demo hour},
  journal = {Interactions},
  year = {2017},
  volume = {25},
  number = {1},
  pages = {8--11},
  month = {Dec},
  issn = {1072-5520},
  day = {21},
  doi = {10.1145/3162013}
}
</pre>

<a name="kudumakis2017dmrn122017"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#kudumakis2017dmrn122017">kudumakis2017dmrn122017</a>,
  author = {KUDUMAKIS, P and SANDLER, M},
  title = {DMRN+12: Digital Music Research Network Workshop Proceedings 2017},
  year = {2017},
  editor = {KUDUMAKIS, P and SANDLER, M},
  month = {Dec},
  organization = {Centre for Digital Music, Queen Mary University of London},
  conference = {DMRN+12: Digital Music Research Network Workshop 2017},
  day = {19},
  doi = {10.26494/DMRN.2017.30583},
  keyword = {DMRN+12: Digital Music Research Network Workshop Proceedings 2017},
  publicationstatus = {online-published},
  startday = {19},
  startmonth = {Dec},
  startyear = {2017}
}
</pre>

<a name="moro2017dynamicsignificance"></a><pre>
@article{<a href="pubs2017_raw.html#moro2017dynamicsignificance">moro2017dynamicsignificance</a>,
  author = {MORO, G and MCPHERSON, A and SANDLER, M},
  title = {Dynamic temporal behaviour of the keyboard action on the Hammond organ and its perceptual significance},
  journal = {Journal of the Acoustical Society of America},
  year = {2017},
  month = {Nov},
  issn = {1520-8524},
  day = {10},
  doi = {10.1121/1.5003796},
  publicationstatus = {published},
  publisher = {Acoustical Society of America}
}
</pre>

<a name="jillings2017zerodelayarchitectures"></a><pre>
@inproceedings{<a href="pubs2017_raw.html#jillings2017zerodelayarchitectures">jillings2017zerodelayarchitectures</a>,
  author = {Jillings, N and Reiss, JD and Stables, R},
  title = {Zero-Delay large signal convolution using multiple processor architectures},
  booktitle = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
  year = {2017},
  volume = {2017-October},
  pages = {339--343},
  month = {Dec},
  abstract = {© 2017 IEEE. Zero latency convolution typically uses the Direct Form approach, requiring a large amount of computational resources for every additional sample in the impulse response. A number of methods have been developed to reduce the computational cost of very large signal convolution. However these all introduce latency into the system. In some scenarios this is not acceptable and must be removed. Modern computer systems hold multiple processor architectures, with their own strengths and weaknesses for the purpose of convolution. This paper shows how correctly combining these processors can lead to a powerful system which can be deployed for real-Time, zero-latency large signal convolution.},
  day = {7},
  doi = {10.1109/WASPAA.2017.8170051},
  isbn = {9781538616321},
  publicationstatus = {published}
}
</pre>

<a name="mcleod2017automaticmusic"></a><pre>
@article{<a href="pubs2017_raw.html#mcleod2017automaticmusic">mcleod2017automaticmusic</a>,
  author = {McLeod, A and Schramm, R and Steedman, M and BENETOS, E},
  title = {Automatic Transcription of Polyphonic Vocal Music},
  journal = {Applied Sciences},
  year = {2017},
  volume = {7},
  number = {1285},
  month = {Dec},
  issn = {2076-3417},
  abstract = {This paper presents a method for automatic music transcription applied to audio recordings of a cappella performances with multiple singers. We propose a system for multi-pitch detection and voice assignment that integrates an acoustic and a music language model. The acoustic model performs spectrogram decomposition, extending probabilistic latent component analysis (PLCA) using a six-dimensional dictionary with pre-extracted log-spectral templates. The music language model performs voice separation and assignment using hidden Markov models that apply musicological assumptions. By integrating the two models, the system is able to detect multiple concurrent pitches in polyphonic vocal music and assign each detected pitch to a specific voice type such as soprano, alto, tenor or bass (SATB). We compare our system against multiple baselines, achieving state-of-the-art results for both multi-pitch detection and voice assignment on a dataset of Bach chorales and another of barbershop quartets. We also present an additional evaluation of our system using varied pitch tolerance levels to investigate its performance at 20-cent pitch resolution.},
  day = {11},
  doi = {10.3390/app7121285},
  issue = {12},
  keyword = {automatic music transcription},
  language = {English},
  publicationstatus = {published},
  publisher = {MDPI AG},
  timestamp = {2018.02.05},
  url = {<a href="http://www.mdpi.com/2076-3417/7/12/1285">http://www.mdpi.com/2076-3417/7/12/1285</a>}
}
</pre>

<a name="panteli2017amusic"></a><pre>
@article{<a href="pubs2017_raw.html#panteli2017amusic">panteli2017amusic</a>,
  author = {PANTELI, M and BENETOS, E and DIXON, S},
  title = {A computational study on outliers in world music},
  journal = {PLoS ONE},
  year = {2017},
  volume = {12},
  number = {e0189399},
  pages = {1--28},
  month = {Dec},
  issn = {1932-6203},
  abstract = {The comparative analysis of world music cultures has been the focus of several ethnomusicological studies in the last century. With the advances of Music Information Retrieval and the increased accessibility of sound archives, large-scale analysis of world music with computational tools is today feasible. We investigate music similarity in a corpus of 8200 recordings of folk and traditional music from 137 countries around the world. In particular, we aim to identify music recordings that are most distinct compared to the rest of our corpus. We refer to these recordings as ‘outliers’. We use signal processing tools to extract music information from audio recordings, data mining to quantify similarity and detect outliers, and spatial statistics to account for geographical correlation. Our findings suggest that Botswana is the country with the most distinct recordings in the corpus and China is the country with the most distinct recordings when considering spatial correlation. Our analysis includes a comparison of musical attributes and styles that contribute to the ‘uniqueness’ of the music of each country.},
  day = {18},
  doi = {10.1371/journal.pone.0189399},
  issue = {12},
  keyword = {world music},
  language = {English},
  publicationstatus = {published},
  publisher = {Public Library of Science (PLoS)},
  timestamp = {2018.02.05},
  url = {<a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0189399">http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0189399</a>}
}
</pre>

<pre>
@comment{{jabref-meta: databaseType:bibtex;}}
</pre>

<hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.98.</em></p>
